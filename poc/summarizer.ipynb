{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ff3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a7755d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(link):\n",
    "#     https://www.youtube.com/watch?v=nt63k3bfXS0&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=6\n",
    "    v = link.find(\"v=\")+2\n",
    "    video_id = link[v:v+11]\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "    return transcript\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c76dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = get_transcript(\"https://www.youtube.com/watch?v=nt63k3bfXS0&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=6\")\n",
    "# transcript = get_transcript(\"https://www.youtube.com/watch?v=bwVrrXk7mtY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf67166-d121-43ef-b462-26c994aeb185",
   "metadata": {},
   "source": [
    "# Preprocessing the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9443f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_paragraphs = \"\"\n",
    "# for words in transcript:\n",
    "#     all_paragraphs += words['text'] + \" \" \n",
    "\n",
    "\n",
    "def preprocess_transcript(transcript):\n",
    "    import re\n",
    "    clean_segments = []\n",
    "    \n",
    "    for segment in transcript:\n",
    "        text = segment['text']\n",
    "        timestamp = segment['start']\n",
    "\n",
    "        # a few common filter words\n",
    "        text = re.sub(r'\\b(uh|um|like|you know|sort of)\\b', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip() \n",
    "        \n",
    "        clean_segments.append({'text': text, 'timestamp': timestamp})\n",
    "    \n",
    "    return clean_segments\n",
    "\n",
    "# cleaned_transcript = preprocess_transcript(transcript)\n",
    "\n",
    "def chunk_by_timestamp(transcript, max_tokens=500):\n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for segment in transcript:\n",
    "        text = segment['text']\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        \n",
    "        if current_length + len(tokens) > max_tokens:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "        current_chunk.append(segment)\n",
    "        current_length += len(tokens)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# chunked_transcript = chunk_by_timestamp(cleaned_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "803249f8-12b4-4a97-9c25-dd6e20791d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def summarize_chunks(chunks):\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        combined_text = \" \".join([seg['text'] for seg in chunk])\n",
    "        timestamp_range = f\"{chunk[0]['timestamp']} - {chunk[-1]['timestamp']}\"\n",
    "        \n",
    "        summary = summarizer(combined_text, max_length=300, min_length=50, do_sample=False)\n",
    "        summaries.append({\n",
    "            'summary': summary[0]['summary_text'],\n",
    "            'timestamp': timestamp_range\n",
    "        })\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "# summaries_with_timestamps = summarize_chunks(chunked_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f3bfce-64de-4aca-9b04-4eeca6189ae6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summaries_with_timestamps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msummaries_with_timestamps\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summaries_with_timestamps' is not defined"
     ]
    }
   ],
   "source": [
    "print(summaries_with_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b5d02-0ee5-4c39-87a7-f7d3f543f6c5",
   "metadata": {},
   "source": [
    "# OpenAI transcript processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e552b88-f0f5-4906-b21f-5d8b9da69e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ef019-8f54-4911-90b1-e9d64d72633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "client.api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a4111a-ae5e-43ff-8248-3d2796e47f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_openai(transcript):\n",
    "    combined_text = \"\"\n",
    "    for segment in transcript:\n",
    "        start_time = segment['start'] / 60\n",
    "        combined_text += f\"[{start_time:.2f}] {segment['text']}\\n\"\n",
    "    return combined_text\n",
    "\n",
    "# cleaned_combined_text = preprocess_for_openai(transcript)\n",
    "# print(cleaned_combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23408c2-83dc-4583-adee-d621a850879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunk(text_to_summarize, model=\"gpt-4o-mini\"):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert at creating detailed, structured lecture notes from transcripts.\n",
    "\n",
    "    Your task is to convert the provided lecture transcript into comprehensive, well-organized, and insightful lecture notes, as well as provide timestamps that align with the video's original transcript.\n",
    "\n",
    "    TeX syntax should be utilized to explain and elaborate on the concepts within each subsection. Generate TeX inline math environments as necessary. \n",
    "    \n",
    "    Each set of notes should follow this format:\n",
    "    \n",
    "    # Lecture Notes on [Topic Name]\n",
    "    \n",
    "    ## Introduction\n",
    "    - Begin with a summary of the overall lecture and its goals.\n",
    "    \n",
    "    ## [Main Section Title]\n",
    "    ### Subsection 1\n",
    "    - Elaborate on the main point(s) of the subsection.\n",
    "    \n",
    "    ### Subsection 2\n",
    "    - Continue explaining and expanding.\n",
    "\n",
    "    ### Additional Analysis\n",
    "    - Expand on the original content, and write a paragraph or two that integrate your own expertise and insights. This should provide an extensive, educational resource on the subject.\n",
    "    \n",
    "    ## [Next Main Section Title]\n",
    "    - Organize subsequent sections similarly, ensuring a logical flow.\n",
    "    \n",
    "    ## Conclusion\n",
    "    - Summarize the lecture's key takeaways.\n",
    "    - Provide general advice or actionable insights related to the topic.\n",
    "    \n",
    "    ### General Tips\n",
    "    - Include any advice or practical steps relevant to understanding or applying the concepts.\n",
    "\n",
    "    Here is the lecture transcript:\n",
    "    \n",
    "    {text_to_summarize}\n",
    "\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": prompt}\n",
    "        ],\n",
    "        seed=0,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "# Generate the summary\n",
    "# summary = summarize_chunk(cleaned_combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb37a47-6d28-446b-941c-d2bcca58eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summary_to_text(summarized, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f535690-22ac-41cd-86fd-25cee50dbe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sanitize_for_latex(text):\n",
    "    \"\"\"\n",
    "    Escapes special LaTeX characters in the given text,\n",
    "    except for braces `{}` used in LaTeX commands.\n",
    "    \"\"\"\n",
    "    special_chars = {\n",
    "        \"#\": r\"\\#\",\n",
    "        \"%\": r\"\\%\",\n",
    "        \"_\": r\"\\_\",\n",
    "        \"&\": r\"\\&\",\n",
    "        \"$\": r\"\\$\",\n",
    "    }\n",
    "    for char, replacement in special_chars.items():\n",
    "        text = text.replace(char, replacement)\n",
    "    return text\n",
    "\n",
    "\n",
    "def save_summary_to_latex(input_text, output_file):\n",
    "    \"\"\"\n",
    "    Converts structured text to a LaTeX file.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The structured text input.\n",
    "        output_file (str): The name of the output .tex file.\n",
    "    \"\"\"\n",
    "    # LaTeX preamble and document start\n",
    "    preamble = r\"\"\"\n",
    "\\documentclass{article}\n",
    "\\usepackage{amsmath}\n",
    "\\usepackage{amssymb}\n",
    "\\usepackage{geometry}\n",
    "\\geometry{margin=1in}\n",
    "\\usepackage{hyperref}\n",
    "\\hypersetup{\n",
    "    colorlinks=true,\n",
    "    linkcolor=blue,\n",
    "    filecolor=magenta,      \n",
    "    urlcolor=cyan,\n",
    "}\n",
    "\n",
    "\\title{Lecture Notes}\n",
    "\\author{}\n",
    "\\date{}\n",
    "\n",
    "\\begin{document}\n",
    "\\maketitle\n",
    "\"\"\"\n",
    "    # End of LaTeX document\n",
    "    end = r\"\\end{document}\"\n",
    "\n",
    "    # Convert headers to LaTeX sections\n",
    "    converted_text = re.sub(r\"^# (.+)$\", r\"\\\\section*{\\1}\", input_text, flags=re.MULTILINE)\n",
    "    converted_text = re.sub(r\"^## (.+)$\", r\"\\\\subsection*{\\1}\", converted_text, flags=re.MULTILINE)\n",
    "    converted_text = re.sub(r\"^### (.+)$\", r\"\\\\subsubsection*{\\1}\", converted_text, flags=re.MULTILINE)\n",
    "\n",
    "\n",
    "    # Escape special LaTeX characters\n",
    "    converted_text = sanitize_for_latex(converted_text)\n",
    "    # Fix math environments\n",
    "\n",
    "    # Handle itemize environments for bullet points\n",
    "    def wrap_itemize(match):\n",
    "        items = match.group(1).strip().split(\"\\n\")\n",
    "        formatted_items = \"\\n\".join([f\"\\\\item {item[2:].strip()}\" for item in items if item.strip()])\n",
    "        return f\"\\\\begin{{itemize}}\\n{formatted_items}\\n\\\\end{{itemize}}\"\n",
    "\n",
    "    # Match groups of lines starting with \"- \"\n",
    "    converted_text = re.sub(r\"(?m)(^- .+(?:\\n- .+)*)\", wrap_itemize, converted_text)\n",
    "\n",
    "    # Ensure math environments are properly handled\n",
    "    converted_text = re.sub(r\"\\\\\\[([^\\\\]+)\\\\\\]\", r\"\\\\[\\1\\\\]\", converted_text)\n",
    "    converted_text = re.sub(r\"\\\\\\((.+?)\\\\\\)\", r\"$\\1$\", converted_text)\n",
    "\n",
    "    # Ensure subscripts and superscripts are properly set in math mode\n",
    "    converted_text = re.sub(r\"([a-zA-Z])_([a-zA-Z0-9]+)\", r\"{\\1_\\{\\2\\}}\", converted_text)\n",
    "    converted_text = re.sub(r\"([a-zA-Z])\\^([a-zA-Z0-9]+)\", r\"{\\1^\\{\\2\\}}\", converted_text)\n",
    "\n",
    "    # Combine parts into the LaTeX document\n",
    "    latex_content = f\"{preamble}{converted_text}\\n{end}\"\n",
    "    # print(latex_content)\n",
    "\n",
    "    # Write to the output file\n",
    "    with open(output_file, 'w') as tex_file:\n",
    "        tex_file.write(latex_content)\n",
    "\n",
    "    print(f\"LaTeX file '{output_file}' generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d674178-6e20-4bdf-85e3-77e386d7f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(summary.choices[0].message.content)\n",
    "save_summary_to_latex(summary.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143900a5-d4b9-4299-9de7-88cab9bbf750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56488957-f5a7-4eff-9482-2f271e8c94b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tex_to_pdf(tex_file, output_dir=None):\n",
    "    if not tex_file.endswith(\".tex\"):\n",
    "        raise ValueError(\"Input file must be a .tex file\")\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(tex_file) or \".\"\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"pdflatex\", \"-interaction=nonstopmode\", \"-file-line-error\", \"-output-directory\", output_dir, tex_file],\n",
    "            check=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "        )\n",
    "        pdf_file = os.path.join(output_dir, os.path.basename(tex_file).replace(\".tex\", \".pdf\"))\n",
    "        if os.path.exists(pdf_file):\n",
    "            print(f\"PDF generated: {pdf_file}\")\n",
    "            return pdf_file\n",
    "        else:\n",
    "            raise FileNotFoundError(\"PDF generation failed. Check the LaTeX log for errors.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error during LaTeX compilation:\")\n",
    "        print(e.stdout)  # Print LaTeX log output for debugging\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15b4ef-6e97-4674-bc77-8e2154949cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tex_file_path = \"./output/summary.tex\"\n",
    "output_pdf = tex_to_pdf(tex_file_path, \"./output/tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa7af89-61eb-498a-839b-b84b2a63e5ec",
   "metadata": {},
   "source": [
    "# Everything in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a9f0d8-adf7-4155-9485-2d406252a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pipeline(link, file_title):\n",
    "    \"\"\"\n",
    "    input: video url\n",
    "    output: summary of the video in pdf form\n",
    "    \"\"\"\n",
    "    print(\"fetching transcript...\")\n",
    "    transcript = get_transcript(link)\n",
    "    print(\"processing transcript...\")\n",
    "    processed_transcript = preprocess_for_openai(transcript)\n",
    "    print(\"summarizing transcript...\")\n",
    "    summarized = summarize_chunk(processed_transcript)\n",
    "    summarized = summarized.choices[0].message.content\n",
    "    print(\"saving to txt...\")\n",
    "    save_summary_to_text(summarized, f'./output/{file_title}.txt')\n",
    "    # output_file = f'{file_title}.tex'\n",
    "    print(\"converting to latex...\")\n",
    "    save_summary_to_latex(summarized, f'./output/{file_title}.tex')\n",
    "    print(\"converting to pdf...\")\n",
    "    tex_to_pdf(f'./output/{file_title}.tex', f'./output/{file_title}')\n",
    "    print(\"complete!\")\n",
    "    \n",
    "# pipeline(\"https://www.youtube.com/watch?v=8NYoQiRANpg&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=7\", 'kernels_lecture')\n",
    "# https://www.youtube.com/watch?v=9-Jl0dxWQs8\n",
    "pipeline(\"https://www.youtube.com/watch?v=9-Jl0dxWQs8\", '3b1b_DL7')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
